# CommVQ í‰ê°€ ê°€ì´ë“œ

ì´ ê°€ì´ë“œëŠ” CommVQ ëª¨ë¸ì„ í‰ê°€í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.

---

## ğŸ“Š ì‚¬ìš© ê°€ëŠ¥í•œ ë²¤ì¹˜ë§ˆí¬

### 1. **LongBench**
- **ì„¤ëª…**: 16ê°œì˜ long-context ì´í•´ ë°ì´í„°ì…‹
- **ì¸¡ì • ì§€í‘œ**: F1, Rouge, Accuracy ë“±
- **ì‹¤í–‰ ì‹œê°„**: ~2-4ì‹œê°„

### 2. **NIAH (Needle in a Haystack)**
- **ì„¤ëª…**: ê¸´ ë¬¸ë§¥ì—ì„œ íŠ¹ì • ì •ë³´ ê²€ìƒ‰ ëŠ¥ë ¥ í…ŒìŠ¤íŠ¸
- **ì¸¡ì • ì§€í‘œ**: Accuracy at different depths and lengths
- **ì‹¤í–‰ ì‹œê°„**: ~1-2ì‹œê°„

### 3. **Memory Measurement**
- **ì„¤ëª…**: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë° ì²˜ë¦¬ëŸ‰ ì¸¡ì •
- **ì¸¡ì • ì§€í‘œ**: Memory usage, throughput
- **ì‹¤í–‰ ì‹œê°„**: ~30ë¶„

### 4. **InfiniteBench**
- **ì„¤ëª…**: ë§¤ìš° ê¸´ ë¬¸ë§¥ (100K+ tokens) ì²˜ë¦¬ ëŠ¥ë ¥
- **ì„œë¸ŒíƒœìŠ¤í¬**: passkey, kv_retrieval, longbook_qa ë“±
- **ì‹¤í–‰ ì‹œê°„**: ~2-3ì‹œê°„ (ì „ì²´), ~30ë¶„ (passkeyë§Œ)

---

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### ë°©ë²• 1: ëª¨ë¸ ë¹„êµ (ì¶”ì²œ)

**ì›ë³¸ CommVQ vs E2E í•™ìŠµ ëª¨ë¸ ë¹„êµ:**

```bash
cd evaluation

# ê¸°ë³¸ ì‚¬ìš© (ìë™ìœ¼ë¡œ ëª¨ë¸ ê²½ë¡œ ê°ì§€)
bash compare_models.sh

# ë˜ëŠ” ëª…ì‹œì ìœ¼ë¡œ ì§€ì •
bash compare_models.sh \
    meta-llama/Llama-3.1-8B-Instruct \
    ../training/finetune/output/llama3.1_8b_int1
```

**ê²°ê³¼:**
- `results_comparison_YYYYMMDD_HHMMSS/` ë””ë ‰í† ë¦¬ì— ëª¨ë“  ê²°ê³¼ ì €ì¥
- ê° ë²¤ì¹˜ë§ˆí¬ë³„ë¡œ original vs e2e ë¹„êµ ë¡œê·¸
- `SUMMARY.txt`ì— ìš”ì•½ ì œê³µ

---

### ë°©ë²• 2: ë‹¨ì¼ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰

**íŠ¹ì • ë²¤ì¹˜ë§ˆí¬ë§Œ ì‹¤í–‰:**

```bash
cd evaluation

# LongBenchë§Œ ì‹¤í–‰
bash run_single_benchmark.sh longbench /path/to/model

# NIAHë§Œ ì‹¤í–‰
bash run_single_benchmark.sh niah /path/to/model

# Memory Measurementë§Œ ì‹¤í–‰
bash run_single_benchmark.sh memory /path/to/model

# InfiniteBenchë§Œ ì‹¤í–‰
bash run_single_benchmark.sh infinitebench /path/to/model

# ëª¨ë“  ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
bash run_single_benchmark.sh all /path/to/model
```

---

## ğŸ“ ê°œë³„ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ (ìˆ˜ë™)

### LongBench

```bash
cd evaluation/longbench

# 1. config/model2path.jsonì— ëª¨ë¸ ì¶”ê°€
# (ë˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ê°€ ìë™ìœ¼ë¡œ ì²˜ë¦¬)

# 2. ì˜ˆì¸¡ ìƒì„±
python pred.py --model llama

# 3. ì ìˆ˜ ê³„ì‚°
python eval.py
```

**ê²°ê³¼ ìœ„ì¹˜:** `pred/` ë° `pred_e/` ë””ë ‰í† ë¦¬

---

### NIAH (Needle in a Haystack)

```bash
cd evaluation/niah

python run_needle_in_haystack.py \
    --model_name meta-llama/Llama-3.1-8B-Instruct \
    --attn_implementation sdpa \
    --s_len 32000 \
    --e_len 64000 \
    --step 16000
```

**íŒŒë¼ë¯¸í„°:**
- `--s_len`: ì‹œì‘ ê¸¸ì´ (í† í°)
- `--e_len`: ì¢…ë£Œ ê¸¸ì´ (í† í°)
- `--step`: ì¦ê°€ ë‹¨ìœ„

**ê²°ê³¼ ìœ„ì¹˜:** `results/` ë””ë ‰í† ë¦¬

---

### Memory Measurement

```bash
cd evaluation/memory_measurement

CUDA_VISIBLE_DEVICES=0 python eval_memory.py \
    --model_name meta-llama/Llama-3.1-8B-Instruct \
    --attn_implementation sdpa
```

**ê²°ê³¼:** í„°ë¯¸ë„ì— ì§ì ‘ ì¶œë ¥ (ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰, ì²˜ë¦¬ ì‹œê°„ ë“±)

---

### InfiniteBench

```bash
cd evaluation/infiniteBench/src

# Passkey íƒœìŠ¤í¬
CUDA_VISIBLE_DEVICES=0 python eval_commvq.py \
    --model_name llama \
    --model_path meta-llama/Llama-3.1-8B-Instruct \
    --task passkey \
    --start_idx 0 \
    --stop_idx 100

# ë‹¤ë¥¸ íƒœìŠ¤í¬ë“¤:
# - kv_retrieval
# - longbook_qa_eng
# - longbook_sum_eng
# - longdialogue_qa_eng
# - math_find
# - number_string
# - code_debug
```

**ê²°ê³¼ ìœ„ì¹˜:** `results/` ë””ë ‰í† ë¦¬

---

## ğŸ”§ ë¬¸ì œ í•´ê²°

### 1. GLIBC ì˜¤ë¥˜
```
Error: GLIBC_2.32 not found
```

**í•´ê²°:** ì´ë¯¸ ëª¨ë“  ìŠ¤í¬ë¦½íŠ¸ì—ì„œ flash-attnì„ ë¹„í™œì„±í™”í•˜ê³  SDPAë¥¼ ì‚¬ìš©í•˜ë„ë¡ ìˆ˜ì •ë˜ì—ˆìŠµë‹ˆë‹¤.

---

### 2. GPU ë©”ëª¨ë¦¬ ë¶€ì¡±
```
RuntimeError: CUDA out of memory
```

**í•´ê²°:**
```bash
# ë‹¨ì¼ GPUë§Œ ì‚¬ìš©
CUDA_VISIBLE_DEVICES=0 python ...

# ë˜ëŠ” batch size ì¤„ì´ê¸° (ì½”ë“œ ìˆ˜ì • í•„ìš”)
```

---

### 3. ëª¨ë¸ ê²½ë¡œ ì˜¤ë¥˜
```
Error: Model not found
```

**í•´ê²°:**
- HuggingFace ëª¨ë¸: `meta-llama/Llama-3.1-8B-Instruct`
- ë¡œì»¬ ëª¨ë¸: ì ˆëŒ€ ê²½ë¡œ ì‚¬ìš© `/home/.../model`
- Fine-tuned ëª¨ë¸: `../training/finetune/output/llama3.1_8b_int1`

---

## ğŸ“Š ê²°ê³¼ í•´ì„

### LongBench ì ìˆ˜

```
Dataset             | Score (F1/Accuracy)
--------------------|--------------------
narrativeqa         | 23.45
qasper              | 43.21
multifieldqa_en     | 52.34
...
Average             | 42.87
```

**ì¢‹ì€ ì ìˆ˜:**
- Average > 40: ìš°ìˆ˜
- Average > 35: ì–‘í˜¸
- Average < 30: ê°œì„  í•„ìš”

---

### NIAH ê²°ê³¼

**ì‹œê°í™”:**
```bash
cd evaluation/niah
python viz.py  # íˆíŠ¸ë§µ ìƒì„±
```

**í•´ì„:**
- Depth 0-100%, Length 32K-64Kì—ì„œì˜ ì •í™•ë„
- ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ (100% = ì™„ë²½)

---

### Memory ê²°ê³¼

```
Average Memory: 12.34 GB
Peak Memory: 15.67 GB
Throughput: 123.45 tokens/sec
```

**ë¹„êµ:**
- CommVQëŠ” baseline ëŒ€ë¹„ ~50% ë©”ëª¨ë¦¬ ì ˆê°
- Throughput 10-20% í–¥ìƒ

---

## ğŸ¯ ì¶”ì²œ í‰ê°€ ìˆœì„œ

### ë¹ ë¥¸ ê²€ì¦ (30ë¶„-1ì‹œê°„)
```bash
# 1. Memory measurement (ê°€ì¥ ë¹ ë¦„)
bash run_single_benchmark.sh memory /path/to/model

# 2. InfiniteBench Passkey (ëŒ€í‘œì )
bash run_single_benchmark.sh infinitebench /path/to/model
```

### ì „ì²´ í‰ê°€ (4-8ì‹œê°„)
```bash
# ëª¨ë“  ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
bash compare_models.sh \
    meta-llama/Llama-3.1-8B-Instruct \
    ../training/finetune/output/llama3.1_8b_int1
```

---

## ğŸ“ˆ ëª¨ë¸ ë¹„êµ ì²´í¬ë¦¬ìŠ¤íŠ¸

í‰ê°€ ì™„ë£Œ í›„ í™•ì¸ ì‚¬í•­:

- [ ] LongBench í‰ê·  ì ìˆ˜ê°€ baseline ìœ ì§€ ë˜ëŠ” í–¥ìƒ
- [ ] NIAHì—ì„œ ê¸´ ë¬¸ë§¥ì—ì„œë„ ë†’ì€ ì •í™•ë„
- [ ] Memory ì‚¬ìš©ëŸ‰ ê°ì†Œ (50% ëª©í‘œ)
- [ ] Throughput ìœ ì§€ ë˜ëŠ” í–¥ìƒ
- [ ] InfiniteBench Passkey > 90% accuracy

---

## ğŸ’¡ íŒ

### ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰

```bash
# nohupìœ¼ë¡œ ì‹¤í–‰
nohup bash compare_models.sh > eval.log 2>&1 &

# ì§„í–‰ ìƒí™© í™•ì¸
tail -f eval.log

# í”„ë¡œì„¸ìŠ¤ í™•ì¸
ps aux | grep python
```

### GPU ì‚¬ìš©ë¥  ëª¨ë‹ˆí„°ë§

```bash
# ë³„ë„ í„°ë¯¸ë„ì—ì„œ
watch -n 1 nvidia-smi
```

### ê²°ê³¼ ë°±ì—…

```bash
# ê²°ê³¼ ì••ì¶•
tar -czf results_$(date +%Y%m%d).tar.gz results_*

# ë‹¤ë¥¸ ì„œë²„ë¡œ ì „ì†¡
scp results_*.tar.gz user@server:/path/
```

---

## ğŸ“š ì°¸ê³  ë¬¸ì„œ

- **LongBench**: https://github.com/THUDM/LongBench
- **InfiniteBench**: https://github.com/OpenBMB/InfiniteBench
- **NIAH**: https://github.com/FranxYao/Long-Context-Data-Engineering
- **CommVQ Paper**: arXiv:2506.18879v1

---

## ğŸ†˜ ë„ì›€ì´ í•„ìš”í•˜ì‹ ê°€ìš”?

ë¬¸ì œê°€ ë°œìƒí•˜ë©´:
1. ë¡œê·¸ íŒŒì¼ í™•ì¸: `results_*/comparison.log`
2. GPU ë©”ëª¨ë¦¬ í™•ì¸: `nvidia-smi`
3. ì˜¤ë¥˜ ë©”ì‹œì§€ ì „ì²´ ë³µì‚¬

