# A100 ì„œë²„ E2E-CommVQ ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ

## í˜„ì¬ ìƒí™© âœ…

- âœ… Step 1 ì™„ë£Œ: KV cache ìˆ˜ì§‘ ì™„ë£Œ (320 files)
- â³ Step 2 ëŒ€ê¸° ì¤‘: Scaling factors ê³„ì‚° í•„ìš”
- â³ Step 3 ëŒ€ê¸° ì¤‘: E2E í•™ìŠµ í•„ìš”

---

## ë‹¤ìŒ ë‹¨ê³„ ì‹¤í–‰

### Step 2: Scaling Factors ê³„ì‚°

```bash
cd /home/ieslab/taehyun/E2E-CommVQ/training

# Scaling factors ê³„ì‚° (5-10ë¶„ ì†Œìš”)
python make_scale.py
```

**ì˜ˆìƒ ì¶œë ¥**:
```
============================================================
Computing scaling factors for E2E-CommVQ
============================================================

Processing 64 layer components (key + value)...
Processing key_000: 10 files
  âœ“ key_000: torch.Size([9934820, 1024])
Processing key_001: 10 files
  âœ“ key_001: torch.Size([9934820, 1024])
...
âš ï¸  No files found for value_000, skipping...
âš ï¸  No files found for value_001, skipping...
...

============================================================
âœ“ Processed: 32 components
  Skipped: 32 components (no data)
  Key layers: 32
  Value layers: 0
âœ“ Saved to: data/learnable_scale.pt
============================================================
```

**í™•ì¸**:
```bash
# Scaling factorsê°€ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸
ls -lh data/learnable_scale.pt
python -c "import torch; d=torch.load('data/learnable_scale.pt'); print('Keys:', list(d.keys())); print('Key layers:', len(d['key']))"
```

---

### Step 3: E2E í•™ìŠµ ì‹œì‘

#### ì˜µì…˜ 1: ë‹¨ì¼ ë ˆì´ì–´ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (ê¶Œì¥ ğŸŒŸ)

```bash
# Layer 0ë§Œ í•™ìŠµ (15ë¶„ ì†Œìš”)
python quantize_key_cache.py 0 \
    --training_method e2e \
    --epochs 50 \
    --lr 0.001 \
    --batch_size 256
```

**ì§„í–‰ ìƒí™©**:
```
Layer 0, Epoch 1/50
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:15<00:00]
Loss: 0.0234, Recon: 0.0189, Commit: 0.0180
Saved best model with loss 0.023400

Layer 0, Epoch 2/50
...
```

#### ì˜µì…˜ 2: ëª¨ë“  ë ˆì´ì–´ í•™ìŠµ

```bash
# ì „ì²´ ìë™í™” ìŠ¤í¬ë¦½íŠ¸ (8-10ì‹œê°„ ì†Œìš”)
bash train_e2e_key_codebook.sh \
    --num_samples 10000 \
    --epochs 50 \
    --lr 0.001 \
    --batch_size 256
```

**ë˜ëŠ” ìˆ˜ë™ìœ¼ë¡œ ê° ë ˆì´ì–´**:
```bash
# ê° ë ˆì´ì–´ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ í•™ìŠµ
for layer in {0..31}; do
    echo "Training layer $layer..."
    python quantize_key_cache.py $layer \
        --training_method e2e \
        --epochs 50 \
        --lr 0.001 \
        --batch_size 256
done
```

---

## ê²°ê³¼ í™•ì¸

### í•™ìŠµëœ ì½”ë“œë¶ í™•ì¸

```bash
# ìƒì„±ëœ ì½”ë“œë¶ íŒŒì¼ í™•ì¸
ls -lh codebook_12bits_128group_21residuals/

# Layer 0 ì½”ë“œë¶ ë‚´ìš© í™•ì¸
python -c "
import torch
data = torch.load('codebook_12bits_128group_21residuals/000_0.pt')
print('Keys:', list(data.keys()))
print('Steps:', len(data['steps']))
print('Final error:', data['final_error'])
"
```

### í‰ê°€ ì¤€ë¹„

í•™ìŠµì´ ì™„ë£Œë˜ë©´:

```bash
# ì½”ë“œë¶ì„ ì ì ˆí•œ ìœ„ì¹˜ë¡œ ë³µì‚¬ (í•„ìš” ì‹œ)
mkdir -p ../evaluation/codebooks/e2e_10k_50epochs
cp -r codebook_12bits_128group_21residuals/* ../evaluation/codebooks/e2e_10k_50epochs/

# í‰ê°€ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
cd ../evaluation/longbench
CUDA_VISIBLE_DEVICES=0 python pred.py \
    --model ../../Llama-3.1-8B-Instruct-CommVQ-E2E
```

---

## ë¬¸ì œ í•´ê²°

### ì˜¤ë¥˜: "No files found for key_XXX"

```bash
# ë°ì´í„° í™•ì¸
python check_data.py

# KV cache ì¬ìˆ˜ì§‘ (í•„ìš” ì‹œ)
python collect_kv_for_e2e.py \
    --model_path meta-llama/Llama-3.1-8B-Instruct \
    --dataset HuggingFaceFW/fineweb-edu \
    --output_dir data/key \
    --num_samples 10000
```

### ì˜¤ë¥˜: "CUDA out of memory"

```bash
# ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
python quantize_key_cache.py 0 \
    --training_method e2e \
    --epochs 50 \
    --lr 0.001 \
    --batch_size 128  # â† 256ì—ì„œ 128ë¡œ ê°ì†Œ
```

### í•™ìŠµ ì¤‘ë‹¨ í›„ ì¬ì‹œì‘

```bash
# ì´ë¯¸ í•™ìŠµëœ ë ˆì´ì–´ëŠ” ê±´ë„ˆë›°ê³  ê³„ì†
for layer in {5..31}; do  # Layer 0-4ëŠ” ì™„ë£Œëœ ìƒíƒœ
    python quantize_key_cache.py $layer \
        --training_method e2e \
        --epochs 50 \
        --lr 0.001 \
        --batch_size 256
done
```

---

## ì˜ˆìƒ ì‹œê°„í‘œ (10,000 ìƒ˜í”Œ ê¸°ì¤€)

| ë‹¨ê³„ | ì†Œìš” ì‹œê°„ | ìƒíƒœ |
|------|----------|------|
| Step 1: KV ìˆ˜ì§‘ | ~30ë¶„ | âœ… ì™„ë£Œ |
| Step 2: Scaling | ~10ë¶„ | â³ ëŒ€ê¸° |
| Step 3a: Layer 0 | ~15ë¶„ | â³ ëŒ€ê¸° |
| Step 3b: ì „ì²´ 32 layers | ~8ì‹œê°„ | â³ ëŒ€ê¸° |

---

## ë‹¤ìŒ ì‹¤í–‰ ëª…ë ¹

```bash
cd /home/ieslab/taehyun/E2E-CommVQ/training

# 1. Scaling factors ê³„ì‚°
python make_scale.py

# 2. Layer 0 í…ŒìŠ¤íŠ¸ (ë¹ ë¥¸ ê²€ì¦)
python quantize_key_cache.py 0 \
    --training_method e2e \
    --epochs 50 \
    --lr 0.001 \
    --batch_size 256

# 3. ê²°ê³¼ê°€ ì¢‹ìœ¼ë©´ ì „ì²´ í•™ìŠµ
bash train_e2e_key_codebook.sh --epochs 50
```

**ì§€ê¸ˆ ë°”ë¡œ ì‹¤í–‰**: `python make_scale.py` ğŸš€

