# E2E-CommVQ Training Flow Guide

## ğŸ¯ EM vs E2E: í•µì‹¬ ì°¨ì´ì 

| ë‹¨ê³„ | EM (ê¸°ì¡´) | E2E (ìƒˆë¡œìš´) |
|------|-----------|--------------|
| **KV ìˆ˜ì§‘** | collect_kv.sh<br>(modeling_llama_collect_kv) | collect_kv_for_e2e.py<br>(ë² ì´ìŠ¤ ëª¨ë¸ ì§ì ‘ ì‚¬ìš©) |
| **Scaling** | make_scale.py | make_scale.py (ë™ì¼) |
| **Key í•™ìŠµ** | quantize_key_cache.py<br>(EM ì•Œê³ ë¦¬ì¦˜) | quantize_key_cache.py<br>(--training_method e2e) |
| **ì¶œë ¥ ë””ë ‰í† ë¦¬** | codebook_12bits_... | codebook_e2e_12bits_... |
| **Value í•™ìŠµ** | finetune.py | finetune.py (ë™ì¼) |

---

## ğŸš€ E2E Training: ì™„ì „ ìë™í™” (ê¶Œì¥)

### ê¸°ë³¸ ì‚¬ìš©ë²•

```bash
cd training

# ëª¨ë“  ê²ƒì„ í•œ ë²ˆì— ì‹¤í–‰
bash train_e2e_key_codebook.sh
```

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ìë™ìœ¼ë¡œ:
1. âœ… KV cache ìˆ˜ì§‘ (1M ìƒ˜í”Œ)
2. âœ… Scaling factors ê³„ì‚°
3. âœ… ëª¨ë“  ë ˆì´ì–´(0-31) Key ì½”ë“œë¶ E2E í•™ìŠµ
4. âœ… ë³„ë„ ë””ë ‰í† ë¦¬ì— ì €ì¥ (`codebook_e2e_12bits_128group_21residuals/`)

### ì»¤ìŠ¤í„°ë§ˆì´ì§•

```bash
# ì»¤ìŠ¤í…€ ëª¨ë¸ ì‚¬ìš©
bash train_e2e_key_codebook.sh \
    --model /path/to/your/model \
    --output_dir my_custom_codebook

# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì •
bash train_e2e_key_codebook.sh \
    --epochs 200 \
    --lr 0.0005 \
    --batch_size 512

# 1-bit ì–‘ìí™”
bash train_e2e_key_codebook.sh \
    --quant_bits 1 \
    --output_dir codebook_e2e_6bits_128group_21residuals

# ë‹¨ì¼ ë ˆì´ì–´ë§Œ í•™ìŠµ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸)
bash train_e2e_key_codebook.sh \
    --layer 0 \
    --epochs 50

# ìƒ˜í”Œ ìˆ˜ ì¡°ì •
bash train_e2e_key_codebook.sh \
    --num_samples 500000  # 50ë§Œ ìƒ˜í”Œë§Œ ì‚¬ìš©
```

### ì „ì²´ ì˜µì…˜

```bash
bash train_e2e_key_codebook.sh --help
```

---

## ğŸ”§ E2E Training: ìˆ˜ë™ ë‹¨ê³„ë³„ ì‹¤í–‰

ë” ì„¸ë°€í•œ ì œì–´ê°€ í•„ìš”í•˜ë‹¤ë©´:

### Step 1: KV Cache ìˆ˜ì§‘

```bash
cd training

python collect_kv_for_e2e.py \
    --model_path meta-llama/Llama-3.1-8B-Instruct \
    --dataset HuggingFaceFW/fineweb-edu \
    --output_dir data/key \
    --num_samples 1000000 \
    --max_seq_length 8192 \
    --quant_bits 2
```

**ì£¼ì˜**: 
- âœ… ë² ì´ìŠ¤ ëª¨ë¸ ì‚¬ìš© (CommVQ í•™ìŠµ ì „)
- âœ… ì¶©ë¶„í•œ ìƒ˜í”Œ ìˆ˜ì§‘ (ìµœì†Œ 100K, ê¶Œì¥ 1M)
- âœ… `data/key/` ë””ë ‰í† ë¦¬ì— ì €ì¥

### Step 2: Scaling Factors ê³„ì‚°

```bash
python make_scale.py
```

**ì¶œë ¥**: `data/learnable_scale.pt`

### Step 3: Key Codebook E2E í•™ìŠµ

#### ì˜µì…˜ A: ëª¨ë“  ë ˆì´ì–´ (ìë™ ìŠ¤í¬ë¦½íŠ¸)

```bash
bash quantize_key_cache_e2e.sh --epochs 100 --lr 0.001
```

#### ì˜µì…˜ B: ë‹¨ì¼ ë ˆì´ì–´ (ìˆ˜ë™)

```bash
# Layer 0
python quantize_key_cache.py 0 \
    --training_method e2e \
    --epochs 100 \
    --lr 0.001 \
    --batch_size 256

# Layer 1
python quantize_key_cache.py 1 \
    --training_method e2e \
    --epochs 100 \
    --lr 0.001 \
    --batch_size 256

# ... (repeat for layers 2-31)
```

**ì¶œë ¥**: `codebook_12bits_128group_21residuals/000_*.pt`, `001_*.pt`, ...

#### ì˜µì…˜ C: ë³„ë„ ë””ë ‰í† ë¦¬ë¡œ ì´ë™

```bash
# í•™ìŠµ í›„
mkdir -p codebook_e2e_12bits_128group_21residuals
mv codebook_12bits_128group_21residuals/*.pt codebook_e2e_12bits_128group_21residuals/
```

### Step 4: Value Codebook í•™ìŠµ (ì„ íƒ)

ValueëŠ” E2Eë¡œ ì´ë¯¸ í•™ìŠµë˜ì–´ ìˆìœ¼ë¯€ë¡œ, ì›í•œë‹¤ë©´ ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©:

```bash
bash finetune/llama3.1_8b_int2.sh
```

---

## ğŸ“ ë””ë ‰í† ë¦¬ êµ¬ì¡°

### EM ë°©ì‹ (ê¸°ì¡´)
```
training/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ key/                    # collect_kv.shë¡œ ìˆ˜ì§‘
â”‚   â””â”€â”€ learnable_scale.pt
â”œâ”€â”€ codebook_12bits_128group_21residuals/  # EM í•™ìŠµ ê²°ê³¼
â”‚   â”œâ”€â”€ 000_0.pt
â”‚   â”œâ”€â”€ 000_1.pt
â”‚   â””â”€â”€ ...
```

### E2E ë°©ì‹ (ìƒˆë¡œìš´)
```
training/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ key/                    # collect_kv_for_e2e.pyë¡œ ìˆ˜ì§‘
â”‚   â””â”€â”€ learnable_scale.pt
â”œâ”€â”€ codebook_e2e_12bits_128group_21residuals/  # E2E í•™ìŠµ ê²°ê³¼ â­
â”‚   â”œâ”€â”€ 000_0.pt
â”‚   â”œâ”€â”€ 000_1.pt
â”‚   â””â”€â”€ ...
```

**ì¤‘ìš”**: E2Eì™€ EM ì½”ë“œë¶ì€ **ë³„ë„ ë””ë ‰í† ë¦¬**ì— ì €ì¥!

---

## âš™ï¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ê°€ì´ë“œ

### ê¸°ë³¸ ì„¤ì • (ê¶Œì¥)
```bash
--epochs 100
--lr 0.001
--batch_size 256
--num_samples 1000000
```

### ë¹ ë¥¸ í”„ë¡œí† íƒ€ì…
```bash
--epochs 50
--lr 0.002
--batch_size 128
--num_samples 100000
--layer 0  # ë‹¨ì¼ ë ˆì´ì–´ë§Œ
```

### ê³ í’ˆì§ˆ í•™ìŠµ
```bash
--epochs 200
--lr 0.0005
--batch_size 512
--num_samples 2000000
```

### GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ
```bash
--batch_size 128  # ë˜ëŠ” 64
--num_samples 500000
```

---

## ğŸ”„ E2E vs EM ì„ íƒ ê°€ì´ë“œ

### E2E ì‚¬ìš© ê¶Œì¥:
- âœ… ë” ì•ˆì •ì ì¸ í•™ìŠµ ì›í•¨
- âœ… Local minima ë¬¸ì œ í•´ê²°í•˜ê³  ì‹¶ìŒ
- âœ… í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ìš©ì´ì„±
- âœ… End-to-end ìµœì í™”

### EM ì‚¬ìš© ê¶Œì¥:
- âœ… ì› ë…¼ë¬¸ê³¼ ì™„ì „íˆ ë™ì¼í•œ ì¬í˜„
- âœ… ë” ë¹ ë¥¸ í•™ìŠµ (100 steps vs 100 epochs)
- âœ… ë©”ëª¨ë¦¬ íš¨ìœ¨ì 

---

## ğŸ§ª í‰ê°€ (E2E vs EM ë¹„êµ)

### E2E ëª¨ë¸ í‰ê°€

```bash
cd evaluation/longbench

# E2E ì½”ë“œë¶ ê²½ë¡œë¥¼ ëª¨ë¸ configì— ì„¤ì • í›„
CUDA_VISIBLE_DEVICES=0 python pred.py --model /path/to/model
```

### ì½”ë“œë¶ êµì²´ ë°©ë²•

```bash
# 1. E2E ì½”ë“œë¶ì„ ëª¨ë¸ ë””ë ‰í† ë¦¬ë¡œ ë³µì‚¬
cp -r codebook_e2e_12bits_128group_21residuals/* \
      /path/to/model/Llama-3.1-8B-Instruct-CommVQ-2bit-codebook/

# 2. í‰ê°€ ì‹¤í–‰
cd evaluation/longbench
CUDA_VISIBLE_DEVICES=0 python pred.py --model /path/to/model
```

---

## â“ FAQ

### Q1: E2Eì™€ EM ì½”ë“œë¶ì„ ê°™ì€ ë””ë ‰í† ë¦¬ì— ì €ì¥í•´ë„ ë˜ë‚˜ìš”?
**A**: âŒ ì•ˆ ë©ë‹ˆë‹¤! íŒŒì¼ ì´ë¦„ì´ ê°™ì•„ì„œ ë®ì–´ì”Œì›Œì§‘ë‹ˆë‹¤. ë°˜ë“œì‹œ ë³„ë„ ë””ë ‰í† ë¦¬ ì‚¬ìš©:
- EM: `codebook_12bits_128group_21residuals/`
- E2E: `codebook_e2e_12bits_128group_21residuals/`

### Q2: collect_kv.shë¥¼ E2Eì—ë„ ì‚¬ìš©í•  ìˆ˜ ìˆë‚˜ìš”?
**A**: âŒ ì•ˆ ë©ë‹ˆë‹¤! E2EëŠ” `collect_kv_for_e2e.py`ë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.
- EM: modeling_llama_collect_kv (í•™ìŠµ ì¤‘)
- E2E: ë² ì´ìŠ¤ ëª¨ë¸ ì§ì ‘ ì‚¬ìš©

### Q3: E2E í•™ìŠµì— ì–¼ë§ˆë‚˜ ê±¸ë¦¬ë‚˜ìš”?
**A**: 
- KV ìˆ˜ì§‘: ~2-4ì‹œê°„ (1M ìƒ˜í”Œ, A100)
- Layerë‹¹ í•™ìŠµ: ~30ë¶„-1ì‹œê°„ (100 epochs, A100)
- ì „ì²´ 32 layers: ~16-32ì‹œê°„

### Q4: ì¤‘ê°„ì— ì¤‘ë‹¨ë˜ë©´ ì–´ë–»ê²Œ í•˜ë‚˜ìš”?
**A**: ë ˆì´ì–´ë³„ë¡œ ì €ì¥ë˜ë¯€ë¡œ, ì´ë¯¸ ì™„ë£Œëœ ë ˆì´ì–´ëŠ” ê±´ë„ˆë›°ê³  ë‚˜ë¨¸ì§€ë§Œ í•™ìŠµ:
```bash
# Layer 10ë¶€í„° ì¬ê°œ
for layer in {10..31}; do
    python quantize_key_cache.py $layer --training_method e2e ...
done
```

### Q5: E2E ê²°ê³¼ê°€ EMë³´ë‹¤ ë‚˜ì˜ë©´?
**A**: í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì • ì‹œë„:
1. Learning rate ì¤„ì´ê¸°: `--lr 0.0005`
2. Epochs ëŠ˜ë¦¬ê¸°: `--epochs 200`
3. Batch size ëŠ˜ë¦¬ê¸°: `--batch_size 512`
4. Commitment loss weight ì¡°ì • (ì½”ë“œ ìˆ˜ì • í•„ìš”)

---

## ğŸ“Š ì˜ˆìƒ ê²°ê³¼

### í•™ìŠµ ê³¡ì„  (Layer 0 ì˜ˆì‹œ)
```
Epoch 1/100 - Loss: 0.5234, Recon: 0.4891, Commit: 0.1372
Epoch 10/100 - Loss: 0.2156, Recon: 0.1912, Commit: 0.0976
Epoch 50/100 - Loss: 0.0834, Recon: 0.0712, Commit: 0.0488
Epoch 100/100 - Loss: 0.0567, Recon: 0.0491, Commit: 0.0304
âœ“ Saved best model with loss 0.0567
```

### ì €ì¥ íŒŒì¼
```
codebook_e2e_12bits_128group_21residuals/
â”œâ”€â”€ 000_0.pt  (Layer 0, Group 0: ~2MB)
â”œâ”€â”€ 000_1.pt  (Layer 0, Group 1: ~2MB)
â”œâ”€â”€ ...
â”œâ”€â”€ 031_7.pt  (Layer 31, Group 7: ~2MB)
â””â”€â”€ (Total: ~512MB for all 256 files)
```

---

## ğŸ“ ì™„ì „í•œ E2E ì˜ˆì‹œ

```bash
#!/bin/bash
# complete_e2e_training.sh

cd training

# 1. ìë™í™” ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš© (ê¶Œì¥)
bash train_e2e_key_codebook.sh \
    --model meta-llama/Llama-3.1-8B-Instruct \
    --dataset HuggingFaceFW/fineweb-edu \
    --output_dir codebook_e2e_12bits_128group_21residuals \
    --quant_bits 2 \
    --epochs 100 \
    --lr 0.001 \
    --batch_size 256 \
    --num_samples 1000000

# 2. (ì„ íƒ) Value í•™ìŠµ
bash finetune/llama3.1_8b_int2.sh

# 3. í‰ê°€
cd ../evaluation/longbench
CUDA_VISIBLE_DEVICES=0 python pred.py --model /path/to/model

echo "âœ“ E2E Training Pipeline Completed!"
```

---

## ğŸ† ì„±ê³µ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] `collect_kv_for_e2e.py`ë¡œ KV cache ìˆ˜ì§‘ ì™„ë£Œ
- [ ] `data/key/`ì— íŒŒì¼ ìƒì„± í™•ì¸
- [ ] `make_scale.py`ë¡œ scaling factors ê³„ì‚°
- [ ] `data/learnable_scale.pt` ì¡´ì¬ í™•ì¸
- [ ] E2E í•™ìŠµ ì™„ë£Œ (ëª¨ë“  ë ˆì´ì–´)
- [ ] `codebook_e2e_12bits_128group_21residuals/`ì— 256ê°œ íŒŒì¼ í™•ì¸
- [ ] í‰ê°€ ìŠ¤í¬ë¦½íŠ¸ ì •ìƒ ì‹¤í–‰
- [ ] ì„±ëŠ¥ ê²°ê³¼ ê¸°ë¡

---

**ë‹¤ìŒ ë‹¨ê³„**: [E2E_COMMVQ_GUIDE.md](E2E_COMMVQ_GUIDE.md)ì—ì„œ ë” ìì„¸í•œ ì •ë³´ í™•ì¸

